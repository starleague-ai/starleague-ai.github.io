<link href="/ui2024/style/css/_general.css" rel="stylesheet" type="text/css" media="all">
<link href="/ui2024/style/css/event.css" rel="stylesheet" type="text/css" media="all">


<body>



    <div class="banner">

    </div>



    <div class="block_left">
        <div style="flex-grow: 3;">

            <h1>Overview</h1>
            <span>
                vberovj mvlewa mvlkerma lmglalv ag lamvlmsa lbleam lvlrmv awv;m  vberovj mvlewa mvlkerma lmglalv ag lamvlmsa lbleam lvlrmv awv;m  vberovj mvlewa mvlkerma lmglalv ag lamvlmsa lbleam lvlrmv awv;m  vberovj mvlewa mvlkerma lmglalv ag lamvlmsa lbleam lvlrmv awv;m  vberovj mvlewa mvlkerma lmglalv ag lamvlmsa lbleam lvlrmv awv;m  vberovj mvlewa mvlkerma lmglalv ag lamvlmsa lbleam lvlrmv awv;m  vberovj mvlewa mvlkerma lmglalv ag lamvlmsa lbleam lvlrmv awv;m  vberovj mvlewa mvlkerma lmglalv ag lamvlmsa lbleam lvlrmv awv;m  vberovj mvlewa mvlkerma lmglalv ag lamvlmsa lbleam lvlrmv awv;m  vberovj mvlewa mvlkerma lmglalv ag lamvlmsa lbleam lvlrmv awv;m  vberovj mvlewa mvlkerma lmglalv ag lamvlmsa lbleam lvlrmv awv;m  vberovj mvlewa mvlkerma lmglalv ag lamvlmsa lbleam lvlrmv awv;m  vberovj mvlewa mvlkerma lmglalv ag lamvlmsa lbleam lvlrmv awv;m  vberovj mvlewa mvlkerma lmglalv ag lamvlmsa lbleam lvlrmv awv;m  vberovj mvlewa mvlkerma lmglalv ag lamvlmsa lbleam lvlrmv awv;m  vberovj mvlewa mvlkerma lmglalv ag lamvlmsa lbleam lvlrmv awv;m  vberovj mvlewa mvlkerma lmglalv ag lamvlmsa lbleam lvlrmv awv;m  vberovj mvlewa mvlkerma lmglalv ag lamvlmsa lbleam lvlrmv awv;m
            </span>
            
            <br>
            <h1>Contact</h1>
            <span>
                <a href="mailto:workshop-e2e-ad@googlegroups.com">
                    <img src="https://img.shields.io/badge/email-Send-blue?logo=gmail&amp"/>
                </a>
                Contact us via <code>workshop-e2e-ad@googlegroups.com</code> with the prefix [CVPR 2024 FM4AS].
            </span>
            <br>
            <span>
                <a href="https://join.slack.com/t/opendrivelab/shared_invite/zt-1rcp42b35-Wc5I0MhUrahlM5qDeJrVqQ" target="_blank">
                    <img src="https://img.shields.io/badge/Slack-Join-blue?logo=slack&amp"/>
                </a>
                Join Slack to chat with Challenge organizers. Please follow the guidelines in <code>#general</code> and join thetrack-specific channels.
            </span>
            
        </div>
        <div style="flex-grow: 1;"></div>
        <div style="flex-grow: 6;">
            <h1 style="user-select: none;">&nbsp;</h1>
            <div style="display: flex; justify-content: center; align-items: center;">
                <div style="min-height: 500px; min-width: 700px; background-color: burlywood;"></div>
            </div>
        </div>
    </div>



    <div class="block">
        <div class="navigator">
            <a>vweajo</a>
            <a>vweajo</a>
        </div>
    </div>



    <div class="block_left" id="openlane_topology">

        <div style="flex-grow: 3;">
            <h1>Track X</h1>
            <a class="track_title" href="#openlane_topology">
                OpenLane Topology
                <img class="link_img"/>
            </a>
        </div>

        <div style="flex-grow: 1;"></div>

        <div style="flex-grow: 6;">

            <span>
                <a href="https://github.com/OpenDriveLab/OpenLane-V2">
                    <img src="https://img.shields.io/badge/website-GitHub-darkgreen" style="height: 150%; margin-right: 1%;"/>
                </a>
                <a href="https://github.com/OpenDriveLab/OpenLane-V2">
                    <img src="https://img.shields.io/github/stars/OpenDriveLab/OpenLane-V2?style=social" style="height: 150%; margin-right: 1%;"/>
                </a>
                <a href="https://github.com/OpenDriveLab/OpenLane-V2">
                    <img src="https://img.shields.io/github/forks/OpenDriveLab/OpenLane-V2?style=social" style="height: 150%; margin-right: 1%;"/>
                </a>
                <a href="https://eval.ai/web/challenges/challenge-page/1925">
                    <img src="https://img.shields.io/badge/submission-EvalAI-darkgreen" style="height: 150%; margin-right: 1%;"/>
                </a>
            </span>

            <br>
            <h2>Task Description</h2>
            <span>
                The OpenLane-V2 dataset* is the perception and reasoning benchmark for scene structure in autonomous driving. Given multi-view images covering the whole panoramic field of view, participants are required to deliver not only perception results of lanes and traffic elements but also topology relationships among lanes and between lanes and traffic elements simultaneously.
                <br><br>
                * The dataset, OpenLane-V2 at Shanghai AI Lab, is named as Road Genome at Huawei and publically as OpenLane-Huawei.
            </span>

            <br>
            <h2>Participation</h2>
            <span>
                The primary metric is <a href="https://github.com/OpenDriveLab/OpenLane-V2">OpenLane-V2</a>  (OLS), which comprises evaluations <a>on three sub-tasks. On the</a> website, we provide tools for data access, training models, evaluations, and visualization. To submit your results on EvalAI, please follow the submission instructions.
            </span>
            
            <br>
            <h2>Date</h2>
            <table style="width: 50%;">
                <tbody>
                    <tr>
                        <td>Challenge Period Open</td><td>March 15, 2023</td>
                    </tr>
                    <tr>
                        <td>Challenge Period End</td><td>June 01, 2023</td>
                    </tr>
                    <tr>
                        <td>Technical Report Deadline</td><td>June 09, 2023</td>
                    </tr>
                    <tr>
                        <td>Winner Announcement</td><td>June 13, 2023</td>
                    </tr>
                </tbody>
            </table>

            <br>
            <h2>Award</h2>
            <table style="width: 50%;">
                <tbody>
                    <tr>
                        <td><img src='/ui2024/style/icon/bulb.png' style="width: 20px; user-select: none; margin-right: 7px;"/>Innovation Award</td><td>USD $5,000</td>
                    </tr>
                    <tr>
                        <td><img src='/ui2024/style/icon/rank01.png' style="width: 20px; user-select: none; margin-right: 7px;"/>Outstanding Champion</td><td>USD $15,000</td>
                    </tr>
                    <tr>
                        <td><img src='/ui2024/style/icon/rank02.png' style="width: 20px; user-select: none; margin-right: 7px;"/>Honorable Runner-up</td><td>USD $5,000</td>
                    </tr>
                </tbody>
            </table>

            <br>
            <h2>Citation</h2>
            <span>
                If you use the challenge dataset in your paper, please consider citing the following BibTex:
            </span>
            <div class="citation">
                @inproceedings{wang2023openlanev2,
                <br>
                &nbsp;&nbsp;&nbsp;&nbsp;title={OpenLane-V2: A Topology Reasoning Benchmark for Unified 3D HD Mapping},
                <br>
                &nbsp;&nbsp;&nbsp;&nbsp;author={Wang, Huijie and Li, Tianyu and Li, Yang and Chen, Li and Sima, Chonghao and Liu, Zhenbo and Wang, Bangjun and Jia, Peijin and Wang, Yuting and Jiang, Shengyin and Wen, Feng and Xu, Hang and Luo, Ping and Yan, Junchi and Zhang, Wei and Li, Hongyang},
                <br>
                &nbsp;&nbsp;&nbsp;&nbsp;booktitle={NeurIPS},
                <br>
                &nbsp;&nbsp;&nbsp;&nbsp;year={2023}
                <br>
                }
            </div>

            <br>
            <h2>Contact</h2>
            <li>
                Huijie Wang (OpenDriveLab), <code>wanghuijie@pjlab.org.cn</code>
            </li>
            <li>
                Slack channel: <code>#openlane-challenge-2023</code>
            </li>

            <br>
            <h2>Related Literature</h2>
            <li>
                <a href="https://arxiv.org/pdf/2304.05277.pdf" target="_blank">
                    Topology Reasoning for Driving Scenes
                </a>
            </li>
            <li>
                <a href="https://arxiv.org/pdf/2304.10440.pdf" target="_blank">
                    OpenLane-V2: A Topology Reasoning Benchmark for Scene Understanding in Autonomous Driving
                </a>
            </li>
            <li>
                <a href="https://arxiv.org/pdf/2203.11089.pdf" target="_blank">
                    PersFormer: 3D Lane Detection via Perspective Transformer and the OpenLane Benchmark
                </a>
            </li>
            <li>
                <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Can_Structured_Birds-Eye-View_Traffic_Scene_Understanding_From_Onboard_Images_ICCV_2021_paper.pdf" target="_blank">
                    Structured Bird's-Eye-View Traffic Scene Understanding From Onboard Images
                </a>
            </li>
            <li>
                <a href="https://arxiv.org/pdf/2208.14437.pdf" target="_blank">
                    MapTR: Structured Modeling and Learning for Online Vectorized HD Map Construction
                </a>
            </li>

        </div>
    </div>


    <div class="block_right" id="openlane_topology">

        <div style="flex-grow: 3;">
            <h1>Track X</h1>
            <a class="track_title" href="#openlane_topology">
                OpenLane Topology
                <img class="link_img"/>
            </a>
        </div>

        <div style="flex-grow: 1;"></div>

        <div style="flex-grow: 6;">

            <span>
                <a href="https://github.com/OpenDriveLab/OpenLane-V2">
                    <img src="https://img.shields.io/badge/website-GitHub-darkgreen" style="height: 150%; margin-right: 1%;"/>
                </a>
                <a href="https://github.com/OpenDriveLab/OpenLane-V2">
                    <img src="https://img.shields.io/github/stars/OpenDriveLab/OpenLane-V2?style=social" style="height: 150%; margin-right: 1%;"/>
                </a>
                <a href="https://github.com/OpenDriveLab/OpenLane-V2">
                    <img src="https://img.shields.io/github/forks/OpenDriveLab/OpenLane-V2?style=social" style="height: 150%; margin-right: 1%;"/>
                </a>
                <a href="https://eval.ai/web/challenges/challenge-page/1925">
                    <img src="https://img.shields.io/badge/submission-EvalAI-darkgreen" style="height: 150%; margin-right: 1%;"/>
                </a>
            </span>

            <br>
            <h2>Task Description</h2>
            <span>
                The OpenLane-V2 dataset* is the perception and reasoning benchmark for scene structure in autonomous driving. Given multi-view images covering the whole panoramic field of view, participants are required to deliver not only perception results of lanes and traffic elements but also topology relationships among lanes and between lanes and traffic elements simultaneously.
                <br><br>
                * The dataset, OpenLane-V2 at Shanghai AI Lab, is named as Road Genome at Huawei and publically as OpenLane-Huawei.
            </span>

            <br>
            <h2>Participation</h2>
            <span>
                The primary metric is <a href="https://github.com/OpenDriveLab/OpenLane-V2">OpenLane-V2</a>  (OLS), which comprises evaluations <a>on three sub-tasks. On the</a> website, we provide tools for data access, training models, evaluations, and visualization. To submit your results on EvalAI, please follow the submission instructions.
            </span>
            
            <br>
            <h2>Date</h2>
            <table style="width: 50%;">
                <tbody>
                    <tr>
                        <td>Challenge Period Open</td><td>March 15, 2023</td>
                    </tr>
                    <tr>
                        <td>Challenge Period End</td><td>June 01, 2023</td>
                    </tr>
                    <tr>
                        <td>Technical Report Deadline</td><td>June 09, 2023</td>
                    </tr>
                    <tr>
                        <td>Winner Announcement</td><td>June 13, 2023</td>
                    </tr>
                </tbody>
            </table>

            <br>
            <h2>Award</h2>
            <table style="width: 50%;">
                <tbody>
                    <tr>
                        <td><img src='/ui2024/style/icon/bulb.png' style="width: 20px; user-select: none; margin-right: 7px;"/>Innovation Award</td><td>USD $5,000</td>
                    </tr>
                    <tr>
                        <td><img src='/ui2024/style/icon/rank01.png' style="width: 20px; user-select: none; margin-right: 7px;"/>Outstanding Champion</td><td>USD $15,000</td>
                    </tr>
                    <tr>
                        <td><img src='/ui2024/style/icon/rank02.png' style="width: 20px; user-select: none; margin-right: 7px;"/>Honorable Runner-up</td><td>USD $5,000</td>
                    </tr>
                </tbody>
            </table>

            <br>
            <h2>Citation</h2>
            <span>
                If you use the challenge dataset in your paper, please consider citing the following BibTex:
            </span>
            <div class="citation">
                @inproceedings{wang2023openlanev2,
                <br>
                &nbsp;&nbsp;&nbsp;&nbsp;title={OpenLane-V2: A Topology Reasoning Benchmark for Unified 3D HD Mapping},
                <br>
                &nbsp;&nbsp;&nbsp;&nbsp;author={Wang, Huijie and Li, Tianyu and Li, Yang and Chen, Li and Sima, Chonghao and Liu, Zhenbo and Wang, Bangjun and Jia, Peijin and Wang, Yuting and Jiang, Shengyin and Wen, Feng and Xu, Hang and Luo, Ping and Yan, Junchi and Zhang, Wei and Li, Hongyang},
                <br>
                &nbsp;&nbsp;&nbsp;&nbsp;booktitle={NeurIPS},
                <br>
                &nbsp;&nbsp;&nbsp;&nbsp;year={2023}
                <br>
                }
            </div>

            <br>
            <h2>Contact</h2>
            <li>
                Huijie Wang (OpenDriveLab), <code>wanghuijie@pjlab.org.cn</code>
            </li>
            <li>
                Slack channel: <code>#openlane-challenge-2023</code>
            </li>

            <br>
            <h2>Related Literature</h2>
            <li>
                <a href="https://arxiv.org/pdf/2304.05277.pdf" target="_blank">
                    Topology Reasoning for Driving Scenes
                </a>
            </li>
            <li>
                <a href="https://arxiv.org/pdf/2304.10440.pdf" target="_blank">
                    OpenLane-V2: A Topology Reasoning Benchmark for Scene Understanding in Autonomous Driving
                </a>
            </li>
            <li>
                <a href="https://arxiv.org/pdf/2203.11089.pdf" target="_blank">
                    PersFormer: 3D Lane Detection via Perspective Transformer and the OpenLane Benchmark
                </a>
            </li>
            <li>
                <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Can_Structured_Birds-Eye-View_Traffic_Scene_Understanding_From_Onboard_Images_ICCV_2021_paper.pdf" target="_blank">
                    Structured Bird's-Eye-View Traffic Scene Understanding From Onboard Images
                </a>
            </li>
            <li>
                <a href="https://arxiv.org/pdf/2208.14437.pdf" target="_blank">
                    MapTR: Structured Modeling and Learning for Online Vectorized HD Map Construction
                </a>
            </li>

        </div>
    </div>
    <div class="block_left" id="openlane_topology">

        <div style="flex-grow: 3;">
            <h1>Track X</h1>
            <a class="track_title" href="#openlane_topology">
                OpenLane Topology
                <img class="link_img"/>
            </a>
        </div>

        <div style="flex-grow: 1;"></div>

        <div style="flex-grow: 6;">

            <span>
                <a href="https://github.com/OpenDriveLab/OpenLane-V2">
                    <img src="https://img.shields.io/badge/website-GitHub-darkgreen" style="height: 150%; margin-right: 1%;"/>
                </a>
                <a href="https://github.com/OpenDriveLab/OpenLane-V2">
                    <img src="https://img.shields.io/github/stars/OpenDriveLab/OpenLane-V2?style=social" style="height: 150%; margin-right: 1%;"/>
                </a>
                <a href="https://github.com/OpenDriveLab/OpenLane-V2">
                    <img src="https://img.shields.io/github/forks/OpenDriveLab/OpenLane-V2?style=social" style="height: 150%; margin-right: 1%;"/>
                </a>
                <a href="https://eval.ai/web/challenges/challenge-page/1925">
                    <img src="https://img.shields.io/badge/submission-EvalAI-darkgreen" style="height: 150%; margin-right: 1%;"/>
                </a>
            </span>

            <br>
            <h2>Task Description</h2>
            <span>
                The OpenLane-V2 dataset* is the perception and reasoning benchmark for scene structure in autonomous driving. Given multi-view images covering the whole panoramic field of view, participants are required to deliver not only perception results of lanes and traffic elements but also topology relationships among lanes and between lanes and traffic elements simultaneously.
                <br><br>
                * The dataset, OpenLane-V2 at Shanghai AI Lab, is named as Road Genome at Huawei and publically as OpenLane-Huawei.
            </span>

            <br>
            <h2>Participation</h2>
            <span>
                The primary metric is <a href="https://github.com/OpenDriveLab/OpenLane-V2">OpenLane-V2</a>  (OLS), which comprises evaluations <a>on three sub-tasks. On the</a> website, we provide tools for data access, training models, evaluations, and visualization. To submit your results on EvalAI, please follow the submission instructions.
            </span>
            
            <br>
            <h2>Date</h2>
            <table style="width: 50%;">
                <tbody>
                    <tr>
                        <td>Challenge Period Open</td><td>March 15, 2023</td>
                    </tr>
                    <tr>
                        <td>Challenge Period End</td><td>June 01, 2023</td>
                    </tr>
                    <tr>
                        <td>Technical Report Deadline</td><td>June 09, 2023</td>
                    </tr>
                    <tr>
                        <td>Winner Announcement</td><td>June 13, 2023</td>
                    </tr>
                </tbody>
            </table>

            <br>
            <h2>Award</h2>
            <table style="width: 50%;">
                <tbody>
                    <tr>
                        <td><img src='/ui2024/style/icon/bulb.png' style="width: 20px; user-select: none; margin-right: 7px;"/>Innovation Award</td><td>USD $5,000</td>
                    </tr>
                    <tr>
                        <td><img src='/ui2024/style/icon/rank01.png' style="width: 20px; user-select: none; margin-right: 7px;"/>Outstanding Champion</td><td>USD $15,000</td>
                    </tr>
                    <tr>
                        <td><img src='/ui2024/style/icon/rank02.png' style="width: 20px; user-select: none; margin-right: 7px;"/>Honorable Runner-up</td><td>USD $5,000</td>
                    </tr>
                </tbody>
            </table>

            <br>
            <h2>Citation</h2>
            <span>
                If you use the challenge dataset in your paper, please consider citing the following BibTex:
            </span>
            <div class="citation">
                @inproceedings{wang2023openlanev2,
                <br>
                &nbsp;&nbsp;&nbsp;&nbsp;title={OpenLane-V2: A Topology Reasoning Benchmark for Unified 3D HD Mapping},
                <br>
                &nbsp;&nbsp;&nbsp;&nbsp;author={Wang, Huijie and Li, Tianyu and Li, Yang and Chen, Li and Sima, Chonghao and Liu, Zhenbo and Wang, Bangjun and Jia, Peijin and Wang, Yuting and Jiang, Shengyin and Wen, Feng and Xu, Hang and Luo, Ping and Yan, Junchi and Zhang, Wei and Li, Hongyang},
                <br>
                &nbsp;&nbsp;&nbsp;&nbsp;booktitle={NeurIPS},
                <br>
                &nbsp;&nbsp;&nbsp;&nbsp;year={2023}
                <br>
                }
            </div>

            <br>
            <h2>Contact</h2>
            <li>
                Huijie Wang (OpenDriveLab), <code>wanghuijie@pjlab.org.cn</code>
            </li>
            <li>
                Slack channel: <code>#openlane-challenge-2023</code>
            </li>

            <br>
            <h2>Related Literature</h2>
            <li>
                <a href="https://arxiv.org/pdf/2304.05277.pdf" target="_blank">
                    Topology Reasoning for Driving Scenes
                </a>
            </li>
            <li>
                <a href="https://arxiv.org/pdf/2304.10440.pdf" target="_blank">
                    OpenLane-V2: A Topology Reasoning Benchmark for Scene Understanding in Autonomous Driving
                </a>
            </li>
            <li>
                <a href="https://arxiv.org/pdf/2203.11089.pdf" target="_blank">
                    PersFormer: 3D Lane Detection via Perspective Transformer and the OpenLane Benchmark
                </a>
            </li>
            <li>
                <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Can_Structured_Birds-Eye-View_Traffic_Scene_Understanding_From_Onboard_Images_ICCV_2021_paper.pdf" target="_blank">
                    Structured Bird's-Eye-View Traffic Scene Understanding From Onboard Images
                </a>
            </li>
            <li>
                <a href="https://arxiv.org/pdf/2208.14437.pdf" target="_blank">
                    MapTR: Structured Modeling and Learning for Online Vectorized HD Map Construction
                </a>
            </li>

        </div>
    </div>
</body>