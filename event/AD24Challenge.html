<link href="/ui2024/style/css/_general.css" rel="stylesheet" type="text/css" media="all">
<link href="/ui2024/style/css/header.css" rel="stylesheet" type="text/css" media="all">
<link href="/ui2024/style/css/footer.css" rel="stylesheet" type="text/css" media="all">
<link href="/ui2024/style/css/event.css" rel="stylesheet" type="text/css" media="all">


<body>



    <div class="banner">

    </div>



    <div class="block_left">
        <div style="flex-grow: 6;">

            <h1>Overview</h1>
            <span>
                vberovj mvlewa mvlkerma lmglalv ag lamvlmsa lbleam lvlrmv awv;m  vberovj mvlewa mvlkerma lmglalv ag lamvlmsa lbleam lvlrmv awv;m  vberovj mvlewa mvlkerma lmglalv ag lamvlmsa lbleam lvlrmv awv;m  vberovj mvlewa mvlkerma lmglalv ag lamvlmsa lbleam lvlrmv awv;m  vberovj mvlewa mvlkerma lmglalv ag lamvlmsa lbleam lvlrmv awv;m  vberovj mvlewa mvlkerma lmglalv ag lamvlmsa lbleam lvlrmv awv;m  vberovj mvlewa mvlkerma lmglalv ag lamvlmsa lbleam lvlrmv awv;m  vberovj mvlewa mvlkerma lmglalv ag lamvlmsa lbleam lvlrmv awv;m  vberovj mvlewa mvlkerma lmglalv ag lamvlmsa lbleam lvlrmv awv;m  vberovj mvlewa mvlkerma lmglalv ag lamvlmsa lbleam lvlrmv awv;m  vberovj mvlewa mvlkerma lmglalv ag lamvlmsa lbleam lvlrmv awv;m  vberovj mvlewa mvlkerma lmglalv ag lamvlmsa lbleam lvlrmv awv;m  vberovj mvlewa mvlkerma lmglalv ag lamvlmsa lbleam lvlrmv awv;m  vberovj mvlewa mvlkerma lmglalv ag lamvlmsa lbleam lvlrmv awv;m  vberovj mvlewa mvlkerma lmglalv ag lamvlmsa lbleam lvlrmv awv;m  vberovj mvlewa mvlkerma lmglalv ag lamvlmsa lbleam lvlrmv awv;m  vberovj mvlewa mvlkerma lmglalv ag lamvlmsa lbleam lvlrmv awv;m  vberovj mvlewa mvlkerma lmglalv ag lamvlmsa lbleam lvlrmv awv;m
            </span>
            
            <br>
            <h1>Contact</h1>
            <span>
                <a href="mailto:workshop-e2e-ad@googlegroups.com">
                    <img src="https://img.shields.io/badge/email-Send-blue?logo=gmail&amp"/>
                </a>
                Contact us via <code>workshop-e2e-ad@googlegroups.com</code> with the prefix [CVPR 2024 FM4AS].
            </span>
            <br>
            <span>
                <a href="https://join.slack.com/t/opendrivelab/shared_invite/zt-1rcp42b35-Wc5I0MhUrahlM5qDeJrVqQ" target="_blank">
                    <img src="https://img.shields.io/badge/Slack-Join-blue?logo=slack&amp"/>
                </a>
                Join Slack to chat with Challenge organizers. Please follow the guidelines in <code>#general</code> and join thetrack-specific channels.
            </span>
            
            <br>
            <h1>Timeline</h1>
            
        </div>
        <div style="flex-grow: 1;"></div>
        <div style="flex-grow: 6;">
            <h2 style="user-select: none;">&nbsp;</h2>
            <div style="display: flex; justify-content: center; align-items: center;">
                <div style="min-height: 450px; min-width: 800px; background-color: burlywood;"></div>
            </div>
        </div>
    </div>



    <div class="block">
        <div class="navigator">
            <a>vweajo</a>
            <a>vweajo</a>
        </div>
    </div>



    <div class="block_left" id="openlane_topology">

        <div style="flex-grow: 7;">
            <a class="track_title" href="#openlane_topology">
                <h2>
                    Track X
                    <img class="link_img"/>
                </h2>
                <h1>OpenLane Topology</h1>
            </a>
            
            <br><br>
            <a href="https://github.com/OpenDriveLab/OpenLane-V2">
                <img src="https://img.shields.io/badge/website-GitHub-darkgreen" style="height: 150%; margin-right: 1%;"/>
            </a>
            <br>
            <a href="https://github.com/OpenDriveLab/OpenLane-V2">
                <img src="https://img.shields.io/github/stars/OpenDriveLab/OpenLane-V2?style=social" style="height: 150%; margin-right: 1%;"/>
            </a>
            <br>
            <a href="https://github.com/OpenDriveLab/OpenLane-V2">
                <img src="https://img.shields.io/github/forks/OpenDriveLab/OpenLane-V2?style=social" style="height: 150%; margin-right: 1%;"/>
            </a>
            <br>
            <a href="https://eval.ai/web/challenges/challenge-page/1925">
                <img src="https://img.shields.io/badge/submission-EvalAI-darkgreen" style="height: 150%; margin-right: 1%;"/>
            </a>
        </div>

        <div style="flex-grow: 1;"></div>

        <div style="flex-grow: 16;">

            <h2>Task Description</h2>
            <span>
                The OpenLane-V2 dataset* is the perception and reasoning benchmark for scene structure in autonomous driving. Given multi-view images covering the whole panoramic field of view, participants are required to deliver not only perception results of lanes and traffic elements but also topology relationships among lanes and between lanes and traffic elements simultaneously.
                <br><br>
                * The dataset, OpenLane-V2 at Shanghai AI Lab, is named as Road Genome at Huawei and publically as OpenLane-Huawei.
            </span>

            <br>
            <h2>Participation</h2>
            <span>
                The primary metric is <a href="https://github.com/OpenDriveLab/OpenLane-V2">OpenLane-V2</a>  (OLS), which comprises evaluations <a>on three sub-tasks. On the</a> website, we provide tools for data access, training models, evaluations, and visualization. To submit your results on EvalAI, please follow the submission instructions.
            </span>
            
            <br>
            <h2>Date</h2>
            <table style="width: 50%;">
                <tbody>
                    <tr>
                        <td>Challenge Period Open</td><td>March 15, 2023</td>
                    </tr>
                    <tr>
                        <td>Challenge Period End</td><td>June 01, 2023</td>
                    </tr>
                    <tr>
                        <td>Technical Report Deadline</td><td>June 09, 2023</td>
                    </tr>
                    <tr>
                        <td>Winner Announcement</td><td>June 13, 2023</td>
                    </tr>
                </tbody>
            </table>

            <br>
            <h2>Award</h2>
            <table style="width: 50%;">
                <tbody>
                    <tr>
                        <td><img src='/ui2024/style/icon/bulb.png' style="width: 20px; user-select: none; margin-right: 7px;"/>Innovation Award</td><td>USD $5,000</td>
                    </tr>
                    <tr>
                        <td><img src='/ui2024/style/icon/rank01.png' style="width: 20px; user-select: none; margin-right: 7px;"/>Outstanding Champion</td><td>USD $15,000</td>
                    </tr>
                    <tr>
                        <td><img src='/ui2024/style/icon/rank02.png' style="width: 20px; user-select: none; margin-right: 7px;"/>Honorable Runner-up</td><td>USD $5,000</td>
                    </tr>
                </tbody>
            </table>

            <br>
            <h2>Citation</h2>
            <span>
                If you use the challenge dataset in your paper, please consider citing the following BibTex:
            </span>
            <div class="citation">
                @inproceedings{wang2023openlanev2,
                <br>
                &nbsp;&nbsp;&nbsp;&nbsp;title={OpenLane-V2: A Topology Reasoning Benchmark for Unified 3D HD Mapping},
                <br>
                &nbsp;&nbsp;&nbsp;&nbsp;author={Wang, Huijie and Li, Tianyu and Li, Yang and Chen, Li and Sima, Chonghao and Liu, Zhenbo and Wang, Bangjun and Jia, Peijin and Wang, Yuting and Jiang, Shengyin and Wen, Feng and Xu, Hang and Luo, Ping and Yan, Junchi and Zhang, Wei and Li, Hongyang},
                <br>
                &nbsp;&nbsp;&nbsp;&nbsp;booktitle={NeurIPS},
                <br>
                &nbsp;&nbsp;&nbsp;&nbsp;year={2023}
                <br>
                }
            </div>

            <br>
            <h2>Contact</h2>
            <li>
                Huijie Wang (OpenDriveLab), <code>wanghuijie@pjlab.org.cn</code>
            </li>
            <li>
                Slack channel: <code>#openlane-challenge-2023</code>
            </li>

            <br>
            <h2>Related Literature</h2>
            <li>
                <a href="https://arxiv.org/pdf/2304.05277.pdf" target="_blank">
                    Topology Reasoning for Driving Scenes
                </a>
            </li>
            <li>
                <a href="https://arxiv.org/pdf/2304.10440.pdf" target="_blank">
                    OpenLane-V2: A Topology Reasoning Benchmark for Scene Understanding in Autonomous Driving
                </a>
            </li>
            <li>
                <a href="https://arxiv.org/pdf/2203.11089.pdf" target="_blank">
                    PersFormer: 3D Lane Detection via Perspective Transformer and the OpenLane Benchmark
                </a>
            </li>
            <li>
                <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Can_Structured_Birds-Eye-View_Traffic_Scene_Understanding_From_Onboard_Images_ICCV_2021_paper.pdf" target="_blank">
                    Structured Bird's-Eye-View Traffic Scene Understanding From Onboard Images
                </a>
            </li>
            <li>
                <a href="https://arxiv.org/pdf/2208.14437.pdf" target="_blank">
                    MapTR: Structured Modeling and Learning for Online Vectorized HD Map Construction
                </a>
            </li>

        </div>
    </div>



    <div class="block_right" id="openlane_topology">

        <div style="flex-grow: 7;">
            <a class="track_title" href="#openlane_topology">
                <h2>
                    Track X
                    <img class="link_img"/>
                </h2>
                <h1>OpenLane Topology</h1>
            </a>
            
            <br><br>
            <a href="https://github.com/OpenDriveLab/OpenLane-V2">
                <img src="https://img.shields.io/badge/website-GitHub-darkgreen" style="height: 150%; margin-right: 1%;"/>
            </a>
            <br>
            <a href="https://github.com/OpenDriveLab/OpenLane-V2">
                <img src="https://img.shields.io/github/stars/OpenDriveLab/OpenLane-V2?style=social" style="height: 150%; margin-right: 1%;"/>
            </a>
            <br>
            <a href="https://github.com/OpenDriveLab/OpenLane-V2">
                <img src="https://img.shields.io/github/forks/OpenDriveLab/OpenLane-V2?style=social" style="height: 150%; margin-right: 1%;"/>
            </a>
            <br>
            <a href="https://eval.ai/web/challenges/challenge-page/1925">
                <img src="https://img.shields.io/badge/submission-EvalAI-darkgreen" style="height: 150%; margin-right: 1%;"/>
            </a>
        </div>

        <div style="flex-grow: 1;"></div>

        <div style="flex-grow: 16;">

            <h2>Task Description</h2>
            <span>
                The OpenLane-V2 dataset* is the perception and reasoning benchmark for scene structure in autonomous driving. Given multi-view images covering the whole panoramic field of view, participants are required to deliver not only perception results of lanes and traffic elements but also topology relationships among lanes and between lanes and traffic elements simultaneously.
                <br><br>
                * The dataset, OpenLane-V2 at Shanghai AI Lab, is named as Road Genome at Huawei and publically as OpenLane-Huawei.
            </span>

            <br>
            <h2>Participation</h2>
            <span>
                The primary metric is <a href="https://github.com/OpenDriveLab/OpenLane-V2">OpenLane-V2</a>  (OLS), which comprises evaluations <a>on three sub-tasks. On the</a> website, we provide tools for data access, training models, evaluations, and visualization. To submit your results on EvalAI, please follow the submission instructions.
            </span>
            
            <br>
            <h2>Date</h2>
            <table style="width: 50%;">
                <tbody>
                    <tr>
                        <td>Challenge Period Open</td><td>March 15, 2023</td>
                    </tr>
                    <tr>
                        <td>Challenge Period End</td><td>June 01, 2023</td>
                    </tr>
                    <tr>
                        <td>Technical Report Deadline</td><td>June 09, 2023</td>
                    </tr>
                    <tr>
                        <td>Winner Announcement</td><td>June 13, 2023</td>
                    </tr>
                </tbody>
            </table>

            <br>
            <h2>Award</h2>
            <table style="width: 50%;">
                <tbody>
                    <tr>
                        <td><img src='/ui2024/style/icon/bulb.png' style="width: 20px; user-select: none; margin-right: 7px;"/>Innovation Award</td><td>USD $5,000</td>
                    </tr>
                    <tr>
                        <td><img src='/ui2024/style/icon/rank01.png' style="width: 20px; user-select: none; margin-right: 7px;"/>Outstanding Champion</td><td>USD $15,000</td>
                    </tr>
                    <tr>
                        <td><img src='/ui2024/style/icon/rank02.png' style="width: 20px; user-select: none; margin-right: 7px;"/>Honorable Runner-up</td><td>USD $5,000</td>
                    </tr>
                </tbody>
            </table>

            <br>
            <h2>Citation</h2>
            <span>
                If you use the challenge dataset in your paper, please consider citing the following BibTex:
            </span>
            <div class="citation">
                @inproceedings{wang2023openlanev2,
                <br>
                &nbsp;&nbsp;&nbsp;&nbsp;title={OpenLane-V2: A Topology Reasoning Benchmark for Unified 3D HD Mapping},
                <br>
                &nbsp;&nbsp;&nbsp;&nbsp;author={Wang, Huijie and Li, Tianyu and Li, Yang and Chen, Li and Sima, Chonghao and Liu, Zhenbo and Wang, Bangjun and Jia, Peijin and Wang, Yuting and Jiang, Shengyin and Wen, Feng and Xu, Hang and Luo, Ping and Yan, Junchi and Zhang, Wei and Li, Hongyang},
                <br>
                &nbsp;&nbsp;&nbsp;&nbsp;booktitle={NeurIPS},
                <br>
                &nbsp;&nbsp;&nbsp;&nbsp;year={2023}
                <br>
                }
            </div>

            <br>
            <h2>Contact</h2>
            <li>
                Huijie Wang (OpenDriveLab), <code>wanghuijie@pjlab.org.cn</code>
            </li>
            <li>
                Slack channel: <code>#openlane-challenge-2023</code>
            </li>

            <br>
            <h2>Related Literature</h2>
            <li>
                <a href="https://arxiv.org/pdf/2304.05277.pdf" target="_blank">
                    Topology Reasoning for Driving Scenes
                </a>
            </li>
            <li>
                <a href="https://arxiv.org/pdf/2304.10440.pdf" target="_blank">
                    OpenLane-V2: A Topology Reasoning Benchmark for Scene Understanding in Autonomous Driving
                </a>
            </li>
            <li>
                <a href="https://arxiv.org/pdf/2203.11089.pdf" target="_blank">
                    PersFormer: 3D Lane Detection via Perspective Transformer and the OpenLane Benchmark
                </a>
            </li>
            <li>
                <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Can_Structured_Birds-Eye-View_Traffic_Scene_Understanding_From_Onboard_Images_ICCV_2021_paper.pdf" target="_blank">
                    Structured Bird's-Eye-View Traffic Scene Understanding From Onboard Images
                </a>
            </li>
            <li>
                <a href="https://arxiv.org/pdf/2208.14437.pdf" target="_blank">
                    MapTR: Structured Modeling and Learning for Online Vectorized HD Map Construction
                </a>
            </li>

        </div>
    </div>



    <div class="block_left" id="openlane_topology">

        <div style="flex-grow: 7;">
            <a class="track_title" href="#openlane_topology">
                <h2>
                    Track X
                    <img class="link_img"/>
                </h2>
                <h1>OpenLane Topology</h1>
            </a>
            
            <br><br>
            <a href="https://github.com/OpenDriveLab/OpenLane-V2">
                <img src="https://img.shields.io/badge/website-GitHub-darkgreen" style="height: 150%; margin-right: 1%;"/>
            </a>
            <br>
            <a href="https://github.com/OpenDriveLab/OpenLane-V2">
                <img src="https://img.shields.io/github/stars/OpenDriveLab/OpenLane-V2?style=social" style="height: 150%; margin-right: 1%;"/>
            </a>
            <br>
            <a href="https://github.com/OpenDriveLab/OpenLane-V2">
                <img src="https://img.shields.io/github/forks/OpenDriveLab/OpenLane-V2?style=social" style="height: 150%; margin-right: 1%;"/>
            </a>
            <br>
            <a href="https://eval.ai/web/challenges/challenge-page/1925">
                <img src="https://img.shields.io/badge/submission-EvalAI-darkgreen" style="height: 150%; margin-right: 1%;"/>
            </a>
        </div>

        <div style="flex-grow: 1;"></div>

        <div style="flex-grow: 16;">

            <h2>Task Description</h2>
            <span>
                The OpenLane-V2 dataset* is the perception and reasoning benchmark for scene structure in autonomous driving. Given multi-view images covering the whole panoramic field of view, participants are required to deliver not only perception results of lanes and traffic elements but also topology relationships among lanes and between lanes and traffic elements simultaneously.
                <br><br>
                * The dataset, OpenLane-V2 at Shanghai AI Lab, is named as Road Genome at Huawei and publically as OpenLane-Huawei.
            </span>

            <br>
            <h2>Participation</h2>
            <span>
                The primary metric is <a href="https://github.com/OpenDriveLab/OpenLane-V2">OpenLane-V2</a>  (OLS), which comprises evaluations <a>on three sub-tasks. On the</a> website, we provide tools for data access, training models, evaluations, and visualization. To submit your results on EvalAI, please follow the submission instructions.
            </span>
            
            <br>
            <h2>Date</h2>
            <table style="width: 50%;">
                <tbody>
                    <tr>
                        <td>Challenge Period Open</td><td>March 15, 2023</td>
                    </tr>
                    <tr>
                        <td>Challenge Period End</td><td>June 01, 2023</td>
                    </tr>
                    <tr>
                        <td>Technical Report Deadline</td><td>June 09, 2023</td>
                    </tr>
                    <tr>
                        <td>Winner Announcement</td><td>June 13, 2023</td>
                    </tr>
                </tbody>
            </table>

            <br>
            <h2>Award</h2>
            <table style="width: 50%;">
                <tbody>
                    <tr>
                        <td><img src='/ui2024/style/icon/bulb.png' style="width: 20px; user-select: none; margin-right: 7px;"/>Innovation Award</td><td>USD $5,000</td>
                    </tr>
                    <tr>
                        <td><img src='/ui2024/style/icon/rank01.png' style="width: 20px; user-select: none; margin-right: 7px;"/>Outstanding Champion</td><td>USD $15,000</td>
                    </tr>
                    <tr>
                        <td><img src='/ui2024/style/icon/rank02.png' style="width: 20px; user-select: none; margin-right: 7px;"/>Honorable Runner-up</td><td>USD $5,000</td>
                    </tr>
                </tbody>
            </table>

            <br>
            <h2>Citation</h2>
            <span>
                If you use the challenge dataset in your paper, please consider citing the following BibTex:
            </span>
            <div class="citation">
                @inproceedings{wang2023openlanev2,
                <br>
                &nbsp;&nbsp;&nbsp;&nbsp;title={OpenLane-V2: A Topology Reasoning Benchmark for Unified 3D HD Mapping},
                <br>
                &nbsp;&nbsp;&nbsp;&nbsp;author={Wang, Huijie and Li, Tianyu and Li, Yang and Chen, Li and Sima, Chonghao and Liu, Zhenbo and Wang, Bangjun and Jia, Peijin and Wang, Yuting and Jiang, Shengyin and Wen, Feng and Xu, Hang and Luo, Ping and Yan, Junchi and Zhang, Wei and Li, Hongyang},
                <br>
                &nbsp;&nbsp;&nbsp;&nbsp;booktitle={NeurIPS},
                <br>
                &nbsp;&nbsp;&nbsp;&nbsp;year={2023}
                <br>
                }
            </div>

            <br>
            <h2>Contact</h2>
            <li>
                Huijie Wang (OpenDriveLab), <code>wanghuijie@pjlab.org.cn</code>
            </li>
            <li>
                Slack channel: <code>#openlane-challenge-2023</code>
            </li>

            <br>
            <h2>Related Literature</h2>
            <li>
                <a href="https://arxiv.org/pdf/2304.05277.pdf" target="_blank">
                    Topology Reasoning for Driving Scenes
                </a>
            </li>
            <li>
                <a href="https://arxiv.org/pdf/2304.10440.pdf" target="_blank">
                    OpenLane-V2: A Topology Reasoning Benchmark for Scene Understanding in Autonomous Driving
                </a>
            </li>
            <li>
                <a href="https://arxiv.org/pdf/2203.11089.pdf" target="_blank">
                    PersFormer: 3D Lane Detection via Perspective Transformer and the OpenLane Benchmark
                </a>
            </li>
            <li>
                <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Can_Structured_Birds-Eye-View_Traffic_Scene_Understanding_From_Onboard_Images_ICCV_2021_paper.pdf" target="_blank">
                    Structured Bird's-Eye-View Traffic Scene Understanding From Onboard Images
                </a>
            </li>
            <li>
                <a href="https://arxiv.org/pdf/2208.14437.pdf" target="_blank">
                    MapTR: Structured Modeling and Learning for Online Vectorized HD Map Construction
                </a>
            </li>

        </div>
    </div>



    <div class="block" style="margin: 0;">
        <div class="navigator"></div>
    </div>
    <div class="block_left" style="margin-bottom: 0; padding-bottom: 0;" id="general_rule">
        <div style="flex-grow: 1;">
            <a class="track_title" href="#general_rule">
                <h1>
                    General Rule
                    <img class="link_img"/>
                </h1>
            </a>
        </div> 
    </div>
    <br>
    <div class="block_left" style="margin-top: 0; padding-top: 0;">
        <div style="flex-grow: 6;">

            <h2>Eligibility</h2>
            <li>
                Participants can form teams of up to 10 persons. A team can participate in multiple tracks and is limited to one submission account. 
            </li>
            <li>
                An individual must be a member of one team and cannot be a member of multiple teams.
            </li>
            <li>
                A participant cannot be involved in the execution or administration of the track.
            </li>
            <li>
                All participants will be provided with certificates. Winners will be asked to present during the CVPR workshop, and may be invited for a tentative report for documenting the challenge.
            </li>
            <li>
                Attempting to hack the test set or similar behaviors will lead to disqualification. 
            </li>

            <br>
            <h2>Technical</h2>
            <li>
                All publicly available datasets and pretrained weights are allowed, while the use of private datasets or pretrained weights is prohibited. The use of data must be described explicitly in the technical report. 
            </li>
            <li>
                No future frame is allowed except where it is stated explicitly. 
            </li>
            <li>
                Only public submissions shown on the leaderboard will be eligible for prizes. Please ensure your result is made public before the deadline and kept public on the leaderboard after the deadline.
            </li>

            <br>
            <h2>Award Selection</h2>
            <li>
                To be eligible for prizes, all participants are required to submit a technical report in PDF format of at most 4 pages (excluding references). All technical reports will be made public after the end of the challenge. 
            </li>
            <li>
                Winners need to provide their code or docker image to organizers for verification if requested. 
            </li>
            <li>
                Organizers have the right to update the rules due to unforeseen circumstances to better serve the mission of the challenge and to disqualify teams who have violated the rules. Organizers reserve all rights for the final explanation.
            </li>
            
        </div>
        <div style="flex-grow: 1;"></div>
        <div style="flex-grow: 6;">
            <h2>FAQ</h2>
        </div>
    </div>



    <div class="block" style="margin: 0;">
        <div class="navigator"></div>
    </div>
    <div class="block_left" style="margin-bottom: 0; padding-bottom: 0;" id="organizer">
        <div style="flex-grow: 1;">
            <a class="track_title" href="#organizer">
                <h1>
                    Organizer
                    <img class="link_img"/>
                </h1>
            </a>
        </div> 
    </div>
    <br>
    <div class="block_left" style="margin-top: 0; padding-top: 0">
        <div style="flex-grow: 1;">

            <h2>General</h2>
            <div class="person_container">
                <div>
                    <a><img src="/ui2024/style/person/huijie_wang.jpg"/></a>
                    <h4>Huijie Wang</h4>
                    <span>vkawsredhvo agvoiae ovk ivwhaeo ndan sahfv w</span>
                    <div></div>
                </div>
                <div>
                    <a><img src="/ui2024/style/person/huijie_wang.jpg"/></a>
                    <h4>Huijie Wang</h4>
                    <span>vkawsredhvo agvoiae ovk ivwhaeo ndan sahfv w</span>
                    <div></div>
                </div>                <div>
                    <a><img src="/ui2024/style/person/huijie_wang.jpg"/></a>
                    <h4>Huijie Wang</h4>
                    <span>vkawsredhvo agvoiae ovk ivwhaeo ndan sahfv w asgvhesaich iv aseihfiw vh iea aseo </span>
                    <div></div>
                </div>                <div>
                    <a><img src="/ui2024/style/person/huijie_wang.jpg"/></a>
                    <h4>Huijie Wang</h4>
                    <span>vkawsredhvo agvo</span>
                    <div></div>
                </div>                <div>
                    <a><img src="/ui2024/style/person/huijie_wang.jpg"/></a>
                    <h4>Huijie Wang</h4>
                    <span>vkawsredhvo agvoiae ovk ivwhaeo ndan sahfv w</span>
                    <div></div>
                </div>                <div>
                    <a><img src="/ui2024/style/person/huijie_wang.jpg"/></a>
                    <h4>Huijie Wang</h4>
                    <span>vkawsredhvo agvoiae ovk ivwhaeo ndan sahfv w</span>
                    <div></div>
                </div>                <div>
                    <a><img src="/ui2024/style/person/huijie_wang.jpg"/></a>
                    <h4>Huijie Wang</h4>
                    <span>vkawsredhvo agvoiae ovk ivwhaeo ndan sahfv w</span>
                    <div></div>
                </div>                <div>
                    <a><img src="/ui2024/style/person/huijie_wang.jpg"/></a>
                    <h4>Huijie Wang</h4>
                    <span>vkawsredhvo agvoiae ovk ivwhaeo ndan sahfv w</span>
                    <div></div>
                </div>                <div>
                    <a><img src="/ui2024/style/person/huijie_wang.jpg"/></a>
                    <h4>Huijie Wang</h4>
                    <span>vkawsredhvo agvoiae ovk ivwhaeo ndan sahfv w</span>
                    <div></div>
                </div>                <div>
                    <a><img src="/ui2024/style/person/huijie_wang.jpg"/></a>
                    <h4>Huijie Wang</h4>
                    <span>vkawsredhvo agvoiae ovk ivwhaeo ndan sahfv w</span>
                    <div></div>
                </div>                <div>
                    <a><img src="/ui2024/style/person/huijie_wang.jpg"/></a>
                    <h4>Huijie Wang</h4>
                    <span>vkawsredhvo agvoiae ovk ivwhaeo ndan sahfv w</span>
                    <div></div>
                </div>                <div>
                    <a><img src="/ui2024/style/person/huijie_wang.jpg"/></a>
                    <h4>Huijie Wang</h4>
                    <span>vkawsredhvo agvoiae ovk ivwhaeo ndan sahfv w</span>
                    <div></div>
                </div>                <div>
                    <a><img src="/ui2024/style/person/huijie_wang.jpg"/></a>
                    <h4>Huijie Wang</h4>
                    <span>vkawsredhvo agvoiae ovk ivwhaeo ndan sahfv w</span>
                    <div></div>
                </div>                <div>
                    <a><img src="/ui2024/style/person/huijie_wang.jpg"/></a>
                    <h4>Huijie Wang</h4>
                    <span>vkawsredhvo agvoiae ovk ivwhaeo ndan sahfv w</span>
                    <div></div>
                </div>                <div>
                    <a><img src="/ui2024/style/person/huijie_wang.jpg"/></a>
                    <h4>Huijie Wang</h4>
                    <span>vkawsredhvo agvoiae ovk ivwhaeo ndan sahfv w</span>
                    <div></div>
                </div>                <div>
                    <a><img src="/ui2024/style/person/huijie_wang.jpg"/></a>
                    <h4>Huijie Wang</h4>
                    <span>vkawsredhvo agvoiae ovk ivwhaeo ndan sahfv w</span>
                    <div></div>
                </div>                <div>
                    <a><img src="/ui2024/style/person/huijie_wang.jpg"/></a>
                    <h4>Huijie Wang</h4>
                    <span>vkawsredhvo agvoiae ovk ivwhaeo ndan sahfv w</span>
                    <div></div>
                </div>                <div>
                    <a><img src="/ui2024/style/person/huijie_wang.jpg"/></a>
                    <h4>Huijie Wang</h4>
                    <span>vkawsredhvo agvoiae ovk ivwhaeo ndan sahfv w</span>
                    <div></div>
                </div>                <div>
                    <a><img src="/ui2024/style/person/huijie_wang.jpg"/></a>
                    <h4>Huijie Wang</h4>
                    <span>vkawsredhvo agvoiae ovk ivwhaeo ndan sahfv w</span>
                    <div></div>
                </div>                <div>
                    <a><img src="/ui2024/style/person/huijie_wang.jpg"/></a>
                    <h4>Huijie Wang</h4>
                    <span>vkawsredhvo agvoiae ovk ivwhaeo ndan sahfv w</span>
                    <div></div>
                </div>                <div>
                    <a><img src="/ui2024/style/person/huijie_wang.jpg"/></a>
                    <h4>Huijie Wang</h4>
                    <span>vkawsredhvo agvoiae ovk ivwhaeo ndan sahfv w</span>
                    <div></div>
                </div>                <div>
                    <a><img src="/ui2024/style/person/huijie_wang.jpg"/></a>
                    <h4>Huijie Wang</h4>
                    <span>vkawsredhvo agvoiae ovk ivwhaeo ndan sahfv w</span>
                    <div></div>
                </div>                <div>
                    <a><img src="/ui2024/style/person/huijie_wang.jpg"/></a>
                    <h4>Huijie Wang</h4>
                    <span>vkawsredhvo agvoiae ovk ivwhaeo ndan sahfv w</span>
                    <div></div>
                </div>                <div>
                    <a><img src="/ui2024/style/person/huijie_wang.jpg"/></a>
                    <h4>Huijie Wang</h4>
                    <span>vkawsredhvo agvoiae ovk ivwhaeo ndan sahfv w</span>
                    <div></div>
                </div>                <div>
                    <a><img src="/ui2024/style/person/huijie_wang.jpg"/></a>
                    <h4>Huijie Wang</h4>
                    <span>vkawsredhvo agvoiae ovk ivwhaeo ndan sahfv w</span>
                    <div></div>
                </div>                <div>
                    <a><img src="/ui2024/style/person/huijie_wang.jpg"/></a>
                    <h4>Huijie Wang</h4>
                    <span>vkawsredhvo agvoiae ovk ivwhaeo ndan sahfv w</span>
                    <div></div>
                </div>
            </div>

        </div> 
    </div>

    <div class="footer">
        <div class="block_footer">

            <br>

            <div class="footer_first_row">
                <a href="https://opendrivelab.com" target="_blank">
                    <img src="/ui2024/style/icon/OpenDriveLab_white.png" class="footer_logo_white"/>
                    <img src="/ui2024/style/icon/OpenDriveLab.png" class="footer_logo_color"/>
                </a>
                <div>
                    <img src="/ui2024/style/icon/OpenDriveLab_white.png" style="width: 200px;"/>
                </div>
            </div>

            <br><br>

            <div class="footer_second_row">
                <div class="logos">
                    <a href="https://github.com/OpenDriveLab" target="_blank"> 
                        <img src="/ui2024/style/icon/github_white.png" class="footer_logo_white"/> 
                        <img src="/ui2024/style/icon/github.png" class="footer_logo_color"/> 
                    </a>
                    <a href="https://twitter.com/OpenDriveLab" target="_blank"> 
                        <img src="/ui2024/style/icon/x_white.png" class="footer_logo_white"/> 
                        <img src="/ui2024/style/icon/x.png" class="footer_logo_color"/> 
                    </a> 
                    <a href="/style/img/qrcode_for_gh_ab31cc47181d_430.jpg" target="_blank"> 
                        <img src="/ui2024/style/icon/wechat_white.png" class="footer_logo_white"/> 
                        <img src="/ui2024/style/icon/wechat.png" class="footer_logo_color"/> 
                    </a>
                    <a href="https://www.zhihu.com/people/PerceptionX" target="_blank"> 
                        <img src="/ui2024/style/icon/zhihu_white.png" class="footer_logo_white"/> 
                        <img src="/ui2024/style/icon/zhihu.png" class="footer_logo_color"/> 
                    </a> 
                    <a href="https://medium.com/@opendrivelab" target="_blank"> 
                        <img src="/ui2024/style/icon/medium_white.png" class="footer_logo_white"/> 
                        <img src="/ui2024/style/icon/medium.png" class="footer_logo_color"/> 
                    </a> 
                    <a href="https://space.bilibili.com/503310953" target="_blank"> 
                        <img src="/ui2024/style/icon/bilibili_white.png" class="footer_logo_white"/> 
                        <img src="/ui2024/style/icon/bilibili.png" class="footer_logo_color"/> 
                    </a> 
                    <a href="https://www.youtube.com/@OpenDriveLab" target="_blank"> 
                        <img src="/ui2024/style/icon/youtube_white.png" class="footer_logo_white"/> 
                        <img src="/ui2024/style/icon/youtube.png" class="footer_logo_color"/> 
                    </a>
                    <a href="https://join.slack.com/t/opendrivelab/shared_invite/zt-1rcp42b35-Wc5I0MhUrahlM5qDeJrVqQ" target="_blank"> 
                        <img src="/ui2024/style/icon/slack_white.png" class="footer_logo_white"/> 
                        <img src="/ui2024/style/icon/slack.png" class="footer_logo_color"/> 
                    </a> 
                    <a href="mailto:contact@opendrivelab.com"> 
                        <img src="/ui2024/style/icon/email_white.png" class="footer_logo_white"/> 
                        <img src="/ui2024/style/icon/email.png" class="footer_logo_color"/> 
                    </a>
                </div>
                <iframe src="https://github.com/sponsors/OpenDriveLab/button" title="Sponsor OpenDriveLab" height="33px" width="128px" style="border: 0; border-radius: 6px;"></iframe>
            </div>

            <br><br>
            <div class="footer_navigator"></div>
            <br>

            <div class="footer_third_row">
                <span>© 2021 - 2024 All Rights Reserved</span>
                <br><br>
                <span style="user-select: text;">上海市徐汇区龙文路129号国际传媒港L1栋10层C区</span>
                <br>
                <span style="user-select: text;">Area C, Floor 10, Building L1, 129 Longwen Rd, Shanghai</span>
                <br>
                <span><a href="https://beian.miit.gov.cn/" target="_blank">沪ICP备2021009351号</a></span>
            </div>

            <br><br>
            
        </div>
    </div>

</body>